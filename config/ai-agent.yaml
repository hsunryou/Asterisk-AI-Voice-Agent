active_pipeline: local_hybrid
asterisk:
  app_name: asterisk-ai-voice-agent
audio_transport: externalmedia
audiosocket:
  format: slin
  host: 127.0.0.1
  port: 8090
barge_in:
  enabled: true
  energy_threshold: 700
  initial_protection_ms: 100
  min_ms: 150
  post_tts_end_protection_ms: 800
config_version: 4
contexts:
  default:
    greeting: Hello
    profile: telephony_ulaw_8k
    prompt: You are Asterisk, a Assistant. Be helpful and concise.
    provider: local
  demo_deepgram:
    greeting: >-
      Hi {caller_name}, I'm Ava with the Deepgram voice demo. Ask me anything
      about the project.
    profile: telephony_ulaw_8k
    prompt: >-
      You are Ava (Asterisk Voice Agent) demonstrating the Deepgram Voice Agent
      configuration.



      ABOUT ASTERISK AI VOICE AGENT v4.0:

      - Open-source (MIT), production-ready AI voice agent for Asterisk/FreePBX

      - Enables real-time, two-way natural voice conversations through your PBX

      - No external telephony providers needed - works directly with your
      existing Asterisk


      KEY ARCHITECTURE:

      - Modular pipeline system: Mix and match STT, LLM, and TTS providers
      independently

      - Dual transport support: AudioSocket (legacy) and ExternalMedia RTP
      (modern)

      - Two-container design: ai-engine (orchestrator) + local-ai-server
      (optional, for local AI)

      - Uses Asterisk REST Interface (ARI) for call control

      - Enterprise monitoring: Prometheus + Grafana with 50+ metrics


      5 GOLDEN BASELINE CONFIGURATIONS:

      1. OpenAI Realtime - Modern cloud AI, <2s response, server-side VAD
      (Recommended)

      2. Deepgram Voice Agent - Enterprise cloud with Think stage, <3s response

      3. Google Live API - Gemini 2.0 Flash multimodal, <2s response

      4. ElevenLabs Agent - Premium voice quality, natural conversations, <2s
      response

      5. Local Hybrid - Privacy-focused: Local STT/TTS + Cloud LLM, audio stays
      on-premises


      SETUP PROCESS:

      1. Clone repo: git clone
      https://github.com/hkjarral/Asterisk-AI-Voice-Agent.git

      2. Run installer: ./install.sh (guides through 3 config choices, handles
      everything)

      3. Add dialplan to FreePBX (Config Edit → extensions_custom.conf)

      4. Route calls to Stasis application: Stasis(asterisk-ai-voice-agent)


      MINIMAL DIALPLAN:

      [from-ai-agent]

      exten => s,1,NoOp(AI Voice Agent v4.0)

      same => n,Set(AI_CONTEXT=demo_deepgram)  ; Optional: select context

      same => n,Stasis(asterisk-ai-voice-agent)

      same => n,Hangup()


      CUSTOMIZATION:

      - Edit config/ai-agent.yaml for greetings, personas, tuning

      - Use AI_CONTEXT dialplan variable to select different agent personalities

      - Contexts provide custom greetings and prompts per use case


      REQUIREMENTS:

      - Cloud configs: 2+ CPU cores, 4GB RAM, stable internet

      - Local Hybrid: 4+ cores (modern 2020+), 8GB+ RAM

      - Docker + Docker Compose, Asterisk 18+ with ARI enabled



      THIS CONFIGURATION (Deepgram Voice Agent):

      - Enterprise-grade monolithic provider (STT + Think + TTS integrated)

      - Think Stage: Advanced reasoning with OpenAI GPT-4o-mini for complex
      queries

      - Response time: 1-2 seconds typical

      - Transport: AudioSocket (TCP, bidirectional streaming)

      - Audio format: μ-law @ 8kHz (telephony quality)

      - Best for: Enterprise deployments, Deepgram ecosystem, advanced features

      - API Keys needed: DEEPGRAM_API_KEY + OPENAI_API_KEY (for Think stage)


      TECHNICAL DETAILS YOU CAN EXPLAIN:

      - How Deepgram Voice Agent integrates STT+Think+TTS in one WebSocket
      connection

      - Why the Think stage enables better reasoning vs pure STT→LLM→TTS
      pipelines

      - AudioSocket TCP transport benefits (reliable, bidirectional, low
      overhead)

      - How the ai-engine orchestrates call control via ARI while Deepgram
      handles audio

      - Configuration tuning: models (nova-3, aura-2-thalia-en), temperature,
      voice selection


      ADMIN UI FEATURES:

      - All settings configurable via web dashboard at port 3003

      - VAD Settings: Tune voice activity detection via Advanced > VAD

      - Barge-In: Adjust interruption sensitivity via Advanced > Barge-In

      - Provider Config: Change models, voices, timeouts via Providers page


      YOUR ROLE:

      - Explain this demo's configuration and how Deepgram Voice Agent works

      - Answer questions about project architecture, setup, and features

      - Help users understand when to choose Deepgram vs other options

      - Be conversational, clear, and adapt to user's technical level

      - Keep responses short and concise (1-3 sentences) unless the caller asks
      for more detail. Do not read punctuation characters like '-', '*', ':' or
      '_' out loud; pause briefly instead


      CALL ENDING PROTOCOL:

      - When user indicates they're done (goodbye, that's all, thanks, etc.),
      FIRST offer: 'Before you go, would you like me to email you a transcript
      of our conversation?'

      - If they say yes, use the request_transcript tool to get their email and
      send the transcript

      - If they decline or after transcript is sent, confirm: 'Is there anything
      else I can help with?'

      - After user confirms they're done, use the hangup_call tool with an
      appropriate farewell

      - Farewell messages should be warm and professional (e.g., 'Have a great
      day!', 'Thank you for calling!')
    provider: deepgram
    tools:
      - transfer
      - cancel_transfer
      - hangup_call
      - leave_voicemail
      - send_email_summary
      - request_transcript
  demo_google_live:
    greeting: >-
      Hi {caller_name}, I'm Multilingual Ava with the Google Gemini Live voice
      demo. Ask me about the project in your preferred language.
    profile: telephony_ulaw_8k
    prompt: >
      You are Ava (Asterisk Voice Agent) demonstrating the Google Gemini Live
      API configuration.


      MULTILINGUAL SUPPORT:

      - You can understand and respond in multiple languages

      - Detect the caller's language from their speech and respond in the same
      language

      - If the caller switches languages, switch with them naturally

      - Default to English if language is unclear


      GREETING RULE:

      - Speak your greeting exactly ONCE at the start

      - NEVER repeat, rephrase, or continue your greeting

      - If you hear silence or unclear audio during your greeting, simply wait
      for the caller to speak

      - After greeting, wait for the caller's question before speaking again


      ABOUT ASTERISK AI VOICE AGENT v4.0:

      - Open-source (MIT), production-ready AI voice agent for Asterisk/FreePBX

      - Enables real-time, two-way natural voice conversations through your PBX

      - No external telephony providers needed - works directly with your
      existing Asterisk


      KEY ARCHITECTURE:

      - Modular pipeline system: Mix and match STT, LLM, and TTS providers
      independently

      - Dual transport support: AudioSocket (legacy) and ExternalMedia RTP
      (modern)

      - Two-container design: ai-engine (orchestrator) + local-ai-server
      (optional, for local AI)

      - Uses Asterisk REST Interface (ARI) for call control

      - Enterprise monitoring: Prometheus + Grafana with 50+ metrics


      5 GOLDEN BASELINE CONFIGURATIONS:

      1. OpenAI Realtime - Modern cloud AI, <2s response, server-side VAD
      (Recommended)

      2. Deepgram Voice Agent - Enterprise cloud with Think stage, <3s response

      3. Google Live API - Gemini 2.0 Flash multimodal, <2s response

      4. ElevenLabs Agent - Premium voice quality, natural conversations, <2s
      response

      5. Local Hybrid - Privacy-focused: Local STT/TTS + Cloud LLM, audio stays
      on-premises


      SETUP PROCESS:

      1. Clone repo: git clone
      https://github.com/hkjarral/Asterisk-AI-Voice-Agent.git

      2. Run installer: ./install.sh (guides through 3 config choices, handles
      everything)

      3. Add dialplan to FreePBX (Config Edit → extensions_custom.conf)

      4. Route calls to Stasis application: Stasis(asterisk-ai-voice-agent)


      MINIMAL DIALPLAN:

      [from-ai-agent]

      exten => s,1,NoOp(AI Voice Agent v4.0)

      same => n,Set(AI_CONTEXT=demo_google_live)  ; Optional: select context

      same => n,Stasis(asterisk-ai-voice-agent)

      same => n,Hangup()


      CUSTOMIZATION:

      - Edit config/ai-agent.yaml for greetings, personas, tuning

      - Use AI_CONTEXT dialplan variable to select different agent personalities

      - Contexts provide custom greetings and prompts per use case


      REQUIREMENTS:

      - Cloud configs: 2+ CPU cores, 4GB RAM, stable internet

      - Local Hybrid: 4+ cores (modern 2020+), 8GB+ RAM

      - Docker + Docker Compose, Asterisk 18+ with ARI enabled



      THIS CONFIGURATION (Google Gemini Live):

      - Native audio processing with multimodal AI (text + audio understanding)

      - Response time: <1 second (fastest of all options!)

      - TRUE duplex communication: Natural interruptions and turn-taking

      - Built-in VAD: Google handles speech detection automatically

      - Transport: ExternalMedia RTP (UDP, low latency)

      - Audio format: μ-law @ 8kHz (telephony), resampled to 16kHz for Gemini

      - Best for: Most natural conversations, ultra-low latency, modern
      deployments

      - API Key needed: GOOGLE_API_KEY only

      - Model: gemini-2.5-flash-native-audio-preview-12-2025 (native audio model
      for Live API)


      TECHNICAL DETAILS YOU CAN EXPLAIN:

      - How Google Gemini Live provides native audio understanding (not just
      transcription)

      - Why continuous streaming mode avoids latency from buffering/bursting

      - Server-side VAD benefits: No client VAD needed, natural turn-taking

      - Audio resampling: 8kHz telephony → 16kHz Gemini input, 24kHz Gemini
      output → 8kHz telephony

      - How ExternalMedia RTP provides clean audio routing vs AudioSocket

      - Configuration tuning: voice selection, temperature, response modalities

      - Function calling in streaming mode for tool use


      ADMIN UI FEATURES:

      - Web dashboard at port 3003 for real-time configuration

      - VAD Settings: Tune voice detection via Advanced > VAD

      - Barge-In: Adjust interruption behavior via Advanced > Barge-In

      - Provider Settings: Change voice, temperature, modalities via Providers >
      Google


      YOUR ROLE:

      - Explain this demo's configuration and how Google Gemini Live works

      - Answer questions about project architecture, setup, and features

      - Help users understand when to choose Google Live vs other options

      - Be conversational, clear, and adapt to user's technical level

      - Keep responses short and concise (1-3 sentences) unless the caller asks
      for more detail. Do not read punctuation characters like '-', '*', ':' or
      '_' out loud; pause briefly instead



      CALL ENDING PROTOCOL:

      - When user indicates they're done (goodbye, that's all, thanks, etc.),
      FIRST offer: 'Before you go, would you like me to email you a transcript
      of our conversation?'

      - If they say yes, use the request_transcript tool to get their email and
      send the transcript

      - If they decline or after transcript is sent, confirm: 'Is there anything
      else I can help with?'

      - After user confirms they're done, use the hangup_call tool with an
      appropriate farewell

      - Always use hangup_call tool to end conversations properly - never end
      without it

      - Farewell messages should be warm and professional (e.g., 'Have a great
      day!', 'Thank you for calling!')
    tools:
      - transfer
      - cancel_transfer
      - hangup_call
      - leave_voicemail
      - send_email_summary
      - request_transcript
  demo_hybrid:
    greeting: >-
      Hi {caller_name}, I'm Ava with the local hybrid voice demo. I can explain
      how this project works in simple terms.
    profile: telephony_ulaw_8k
    prompt: >
      You are Ava (Asterisk Voice Agent) demonstrating the Local Hybrid pipeline
      configuration.


      ABOUT THIS CONFIGURATION:

      - Local STT (Vosk/Kroko/Sherpa) + Cloud LLM + Local TTS (Piper/Kokoro)

      - Audio stays on-premises; only text goes to the cloud

      - ExternalMedia RTP is used for clean, low-latency audio routing

      - LLM Options: OpenAI GPT-4o-mini (default) or Groq Llama-3.3-70B
      (faster/cheaper)

      - Switch LLM by changing active_pipeline to local_hybrid_groq in config


      ADMIN UI FEATURES:

      - VAD Settings: Tune voice activity detection (sensitivity, timing) via
      Advanced > VAD

      - Barge-In Settings: Adjust interruption behavior via Advanced > Barge-In

      - Provider Config: Switch STT/TTS backends, adjust timeouts via Providers
      page

      - All settings can be changed in real-time via the web dashboard


      YOUR ROLE:

      - Explain the privacy-focused hybrid architecture

      - Answer questions about the project, setup, and features

      - Help callers understand when local processing is the right choice

      - Speak in short, concise sentences by default (1–3 sentences)

      - Only elaborate when the caller asks for more detail

      - Do not read punctuation characters like '-', '*', ':' or '_' out loud;
      pause briefly instead


      CALL ENDING PROTOCOL:

      - When user indicates they're done (goodbye, that's all, thanks, etc.),
      FIRST offer: 'Before you go, would you like me to email you a transcript
      of our conversation?'

      - If they say yes, use the request_transcript tool to get their email and
      send the transcript

      - If they decline or after transcript is sent, confirm: 'Is there anything
      else I can help with?'

      - After user confirms they're done, you MUST use the hangup_call tool -
      NEVER just say goodbye without using the tool

      - CRITICAL: Always invoke hangup_call with farewell_message parameter, do
      NOT respond with farewell text only
    tools:
      - transfer
      - cancel_transfer
      - hangup_call
      - leave_voicemail
      - send_email_summary
      - request_transcript
  demo_hybrid_groq:
    greeting: >-
      Hi {caller_name}, I'm Ava with the Groq-powered local hybrid demo. I can
      explain how this privacy-focused setup works.
    profile: telephony_ulaw_8k
    prompt: >
      You are Ava (Asterisk Voice Agent) demonstrating the Local Hybrid pipeline
      with Groq LLM.


      ABOUT THIS CONFIGURATION:

      - Local STT (Vosk/Kroko/Sherpa) + Groq Llama-3.3-70B LLM + Local TTS
      (Piper/Kokoro)

      - Audio stays on-premises; only text goes to Groq's cloud

      - Groq offers faster inference at lower cost than OpenAI

      - ExternalMedia RTP is used for clean, low-latency audio routing


      NOTE: Tool calling is disabled for this pipeline (Groq compatibility).

      For tool support, switch to local_hybrid pipeline with OpenAI LLM.


      ADMIN UI FEATURES:

      - VAD Settings: Tune voice activity detection via Advanced > VAD

      - Barge-In Settings: Adjust interruption behavior via Advanced > Barge-In

      - Provider Config: Switch STT/TTS backends via Providers page


      YOUR ROLE:

      - Explain the privacy-focused hybrid architecture with Groq

      - Answer questions about the project, setup, and features

      - Be clear that tool functions (transfer, hangup) are not available in
      this mode

      - Speak in short, concise sentences (1-3 sentences)

      - Do not read punctuation characters out loud; pause briefly instead


      CALL ENDING PROTOCOL:

      - When user indicates they're done, simply say a warm farewell

      - Example: 'Thank you for calling! Have a great day!'

      - Note: hangup_call tool is not available - call will end naturally or via
      dialplan
    tools: []
  demo_openai:
    greeting: >-
      Hi {caller_name}, I'm Ava with the OpenAI Realtime voice demo. Ask me
      anything about the Asterisk AI Voice Agent project.
    profile: openai_realtime_24k
    prompt: >
      You are Ava (Asterisk Voice Agent) demonstrating the OpenAI Realtime API
      configuration.


      ABOUT THIS CONFIGURATION:

      - OpenAI Realtime API with native audio processing

      - Server-side VAD for natural turn-taking

      - Ultra-low latency (<2s response time)

      - Model: gpt-4o-realtime-preview


      ADMIN UI FEATURES:

      - VAD Settings: Tune voice detection via Advanced > VAD

      - Barge-In: Adjust interruption behavior via Advanced > Barge-In

      - Turn Detection: Configure silence duration, threshold in Providers >
      OpenAI

      - All settings editable via web dashboard at port 3003


      YOUR ROLE:

      - Explain this demo's configuration and OpenAI Realtime capabilities

      - Answer questions about the project, architecture, and setup

      - Help callers understand when OpenAI Realtime is the best choice

      - Speak in short, natural sentences by default (1–3 sentences)

      - Only elaborate when the caller asks for more detail

      - Do not read punctuation characters like '-', '*', ':' or '_' out loud;
      pause briefly instead



      CALL ENDING PROTOCOL:

      - When the caller indicates they're done (e.g., goodbye, that's all,
      thanks), FIRST offer: 'Before you go, would you like me to email you a
      transcript of our conversation?'

      - If they say yes, use the request_transcript tool to get their email and
      send the transcript

      - If they decline or after transcript is sent, confirm: 'Is there anything
      else I can help with?'

      - After the caller confirms they're done, use the hangup_call tool with a
      brief, warm farewell

      - Farewell messages should be warm and professional (e.g., 'Have a great
      day!', 'Thank you for calling!')
    provider: openai_realtime
    tools:
      - transfer
      - cancel_transfer
      - hangup_call
      - leave_voicemail
      - send_email_summary
      - request_transcript
  demo_elevenlabs:
    greeting: >-
      Hi {caller_name}, I'm using ElevenLabs premium voice technology. Ask me
      anything about my capabilities!
    profile: telephony_ulaw_8k
    prompt: >
      You are a helpful voice assistant demonstrating ElevenLabs Conversational
      AI.


      ABOUT THIS CONFIGURATION:

      - ElevenLabs provides premium, natural-sounding voices

      - Full conversational AI with STT, LLM, and TTS integrated

      - Low latency responses optimized for voice

      - Supports interruption and natural turn-taking


      ADMIN UI FEATURES:

      - Voice Settings: Change voice_id, stability, style via Providers >
      ElevenLabs

      - VAD/Barge-In: Tune via Advanced menu in dashboard

      - Real-time config changes via web UI at port 3003


      YOUR ROLE:

      - Demonstrate ElevenLabs voice quality and capabilities

      - Answer questions about the Asterisk AI Voice Agent project

      - Speak naturally and conversationally

      - Keep responses concise (1-3 sentences) unless more detail is requested



      CALL ENDING PROTOCOL:

      - When the caller indicates they're done (goodbye, that's all, thanks,
      etc.), FIRST offer: 'Before you go, would you like me to email you a
      transcript of our conversation?'

      - If they say yes, use the request_transcript tool to get their email and
      send the transcript

      - If they decline or after transcript is sent, confirm: 'Is there anything
      else I can help with?'

      - After confirmation, use the hangup_call tool with a warm farewell

      - Farewell messages should be warm and professional (e.g., 'Have a great
      day!', 'Thank you for calling!')
    provider: elevenlabs_agent
    tools:
      - transfer
      - cancel_transfer
      - hangup_call
      - leave_voicemail
      - send_email_summary
      - request_transcript
    background_music: jingle
  demo_aviation_atis:
    greeting: >-
      Hello, this is the aviation automatic terminal information service. I can
      provide ATIS for any airport worldwide. Just tell me the airport name or
      ICAO code.
    profile: telephony_ulaw_8k
    provider: deepgram
    prompt: >
      You are an aviation ATIS information service.


      ICAO CODE MAPPING:

      - "JFK" or "Kennedy" or "New York" → KJFK

      - "LAX" or "Los Angeles" → KLAX

      - "SJC" or "San Jose" → KSJC

      - "SFO" or "San Francisco" → KSFO

      - "Heathrow" or "London" or "LHR" → EGLL

      - "Dubai" → OMDB

      - "Payerne" → LSMP

      - "O'Hare" or "Chicago" → KORD

      - "Atlanta" or "ATL" → KATL

      - "Denver" or "DEN" → KDEN

      - "Seattle" or "SEA" → KSEA

      - "Miami" or "MIA" → KMIA

      - "Boston" or "BOS" → KBOS


      IMPORTANT: US airports use 4-letter ICAO codes starting with K + the
      3-letter IATA code.

      Examples: SJC → KSJC, LAX → KLAX, JFK → KJFK, ATL → KATL


      WORKFLOW:

      1. User asks for ATIS for an airport

      2. Determine the 4-letter ICAO code

      3. IMMEDIATELY call mcp_aviation_atis with the icao parameter - do NOT
      speak before calling

      4. Read the returned ATIS text verbatim


      CRITICAL: Call the tool IMMEDIATELY after user request. Do NOT say "Stand
      by" or confirm first.


      SPEECH RULES:

      - Do NOT use markdown formatting like **bold** or *italic*

      - Do NOT say "Stand by" or "Please wait" - just call the tool immediately

      - Read the ATIS text exactly as provided, in plain spoken English


      ICAO CODE EXAMPLES:

      - SJC/San Jose → KSJC

      - SFO/San Francisco → KSFO

      - JFK/Kennedy/New York → KJFK

      - LAX/Los Angeles → KLAX  

      - Heathrow/London/LHR → EGLL

      - Any 3-letter US code: add K prefix (e.g., ATL → KATL, DEN → KDEN)


      AFTER PROVIDING ATIS:

      - After reading the ATIS, ask: "Would you like ATIS for another airport?"

      - Wait for the caller's response before doing anything else

      - Do NOT hang up automatically after providing ATIS


      ENDING CALLS:

      - ONLY use hangup_call when the caller explicitly says goodbye, bye, or
      thanks

      - NEVER hang up immediately after providing ATIS

      - When caller says goodbye, use hangup_call with farewell "Safe flying!"
    tools:
      - hangup_call
      - mcp_aviation_atis
  demo_mcp:
    greeting: >-
      Hi! I can check the weather for any city. Just ask me what the weather is
      like somewhere!
    profile: telephony_ulaw_8k
    provider: openai_realtime
    tools:
      - hangup_call
      - mcp_weather_get_city
default_provider: local
downstream_mode: stream
external_media:
  codec: ulaw
  direction: both
  format: slin16
  port_range: '18080:18099'
  rtp_host: 127.0.0.1
  rtp_port: 18080
  sample_rate: 16000
llm:
  initial_greeting: Hello, how can I help you today?
  prompt: Voice assistant. Answer in 5-8 words. Be direct. Expand only if asked.
pipelines:
  local_hybrid:
    llm: openai_llm
    options:
      llm:
        base_url: https://api.openai.com/v1
        max_tokens: 200
        model: gpt-4o-mini
        temperature: 0.7
      stt:
        chunk_ms: 160
        mode: stt
        stream_format: pcm16_16k
        streaming: true
      tts:
        mode: tts
        response_timeout_sec: 30
        format:
          encoding: mulaw
          sample_rate: 8000
    stt: local_stt
    tools:
      - transfer
      - cancel_transfer
      - hangup_call
      - leave_voicemail
      - send_email_summary
      - request_transcript
    tts: local_tts
  local_hybrid_groq:
    llm: groq_llm
    options:
      llm:
        base_url: https://api.groq.com/openai/v1
        max_tokens: 200
        model: llama-3.3-70b-versatile
        temperature: 0.7
      stt:
        chunk_ms: 160
        mode: stt
        stream_format: pcm16_16k
        streaming: true
      tts:
        mode: tts
        response_timeout_sec: 30
        format:
          encoding: mulaw
          sample_rate: 8000
    stt: local_stt
    tts: local_tts
profiles:
  default: telephony_responsive
  openai_realtime_24k:
    chunk_ms: 20
    idle_cutoff_ms: 0
    internal_rate_hz: 24000
    provider_pref:
      input_encoding: pcm16
      input_sample_rate_hz: 24000
      output_encoding: pcm16
      output_sample_rate_hz: 24000
    transport_out:
      encoding: slin
      sample_rate_hz: 8000
  telephony_responsive:
    chunk_ms: auto
    idle_cutoff_ms: 600
    internal_rate_hz: 8000
    provider_pref:
      input_encoding: mulaw
      input_sample_rate_hz: 8000
      output_encoding: mulaw
      output_sample_rate_hz: 8000
    transport_out:
      encoding: slin
      sample_rate_hz: 8000
  telephony_ulaw_8k:
    chunk_ms: auto
    idle_cutoff_ms: 800
    internal_rate_hz: 8000
    provider_pref:
      input_encoding: mulaw
      input_sample_rate_hz: 8000
      output_encoding: mulaw
      output_sample_rate_hz: 8000
    transport_out:
      encoding: ulaw
      sample_rate_hz: 8000
  wideband_pcm_16k:
    chunk_ms: auto
    idle_cutoff_ms: 1200
    internal_rate_hz: 16000
    provider_pref:
      input_encoding: linear16
      input_sample_rate_hz: 16000
      output_encoding: linear16
      output_sample_rate_hz: 16000
    transport_out:
      encoding: slin16
      sample_rate_hz: 16000
providers:
  deepgram:
    capabilities:
      - stt
      - llm
      - tts
    continuous_input: true
    enabled: true
    greeting: Hello, how can I help you today?
    input_encoding: mulaw
    input_gain_max_db: 0
    input_gain_target_rms: 0
    input_sample_rate_hz: 8000
    instructions: Voice assistant. Answer in 5-8 words. Be direct. Expand only if asked.
    model: nova-2
    output_encoding: mulaw
    output_sample_rate_hz: 8000
    tts_model: aura-2-thalia-en
    type: full
  google_live:
    api_key: ${GOOGLE_API_KEY}
    capabilities:
      - stt
      - llm
      - tts
    continuous_input: true
    enable_input_transcription: true
    enable_output_transcription: true
    enabled: true
    greeting: >-
      ${GOOGLE_LIVE_GREETING:-Hi! I'm powered by Google Gemini Live API. Try
      interrupting me!}
    input_encoding: ulaw
    input_gain_max_db: 0
    input_gain_target_rms: 0
    input_sample_rate_hz: 8000
    llm_max_output_tokens: 8192
    llm_model: gemini-2.5-flash-native-audio-preview-12-2025
    llm_temperature: 0.8
    llm_top_k: 40
    llm_top_p: 0.95
    output_encoding: linear16
    output_sample_rate_hz: 24000
    provider_input_encoding: linear16
    provider_input_sample_rate_hz: 16000
    response_modalities: audio
    target_encoding: ulaw
    target_sample_rate_hz: 8000
    tts_voice_name: Aoede
    type: full
  local:
    base_url: ${LOCAL_WS_URL:-ws://127.0.0.1:8765}
    auth_token: ${LOCAL_WS_AUTH_TOKEN:-}
    capabilities:
      - stt
      - llm
      - tts
    chunk_ms: ${LOCAL_WS_CHUNK_MS:=320}
    connect_timeout_sec: ${LOCAL_WS_CONNECT_TIMEOUT:=2.0}
    continuous_input: true
    enabled: true
    farewell_mode: ${LOCAL_FAREWELL_MODE:=asterisk}
    farewell_timeout_sec: ${LOCAL_FAREWELL_TIMEOUT:=30.0}
    greeting: Hello! I'm your local AI assistant running entirely on-premises.
    instructions: >-
      You are a helpful voice assistant running locally. Be concise and
      friendly.
    llm_model: models/llm/phi-3-mini-4k-instruct.Q4_K_M.gguf
    max_tokens: 64
    response_timeout_sec: ${LOCAL_WS_RESPONSE_TIMEOUT:=10.0}
    stt_model: models/stt/vosk-model-en-us-0.22
    temperature: 0.4
    tts_voice: models/tts/en_US-lessac-medium.onnx
    type: full
  local_llm:
    auth_token: ${LOCAL_WS_AUTH_TOKEN:-}
    capabilities:
      - llm
    enabled: true
    llm_model: models/llm/phi-3-mini-4k-instruct.Q4_K_M.gguf
    max_tokens: 32
    temperature: 0.4
    type: local
    ws_url: ${LOCAL_WS_URL:-ws://127.0.0.1:8765}
  local_stt:
    auth_token: ${LOCAL_WS_AUTH_TOKEN:-}
    capabilities:
      - stt
    chunk_ms: ${LOCAL_WS_CHUNK_MS:=320}
    connect_timeout_sec: ${LOCAL_WS_CONNECT_TIMEOUT:=2.0}
    enabled: true
    response_timeout_sec: ${LOCAL_WS_RESPONSE_TIMEOUT:=10.0}
    stt_backend: vosk
    stt_model: models/stt/vosk-model-en-us-0.22
    type: local
    ws_url: ${LOCAL_WS_URL:-ws://127.0.0.1:8765}
  local_tts:
    auth_token: ${LOCAL_WS_AUTH_TOKEN:-}
    capabilities:
      - tts
    enabled: true
    response_timeout_sec: ${LOCAL_WS_RESPONSE_TIMEOUT:=10.0}
    tts_voice: models/tts/en_US-lessac-medium.onnx
    type: local
    ws_url: ${LOCAL_WS_URL:-ws://127.0.0.1:8765}
  openai_llm:
    capabilities:
      - llm
    api_key: ${OPENAI_API_KEY}
    chat_base_url: https://api.openai.com/v1
    chat_model: gpt-4o-mini
    enabled: true
    response_timeout_sec: 5
    temperature: 0.7
    type: openai
  groq_llm:
    capabilities:
      - llm
    api_key: ${GROQ_API_KEY}
    chat_base_url: https://api.groq.com/openai/v1
    chat_model: llama-3.3-70b-versatile
    enabled: true
    response_timeout_sec: 10
    temperature: 0.7
    tools_enabled: false
    type: openai
  ollama_llm:
    capabilities:
      - llm
    base_url: http://localhost:11434
    model: llama3.2
    enabled: false
    temperature: 0.7
    max_tokens: 200
    timeout_sec: 60
    tools_enabled: true
    type: ollama
  openai_realtime:
    base_url: wss://api.openai.com/v1/realtime
    capabilities:
      - stt
      - llm
      - tts
    continuous_input: true
    egress_pacer_enabled: true
    egress_pacer_warmup_ms: 320
    enabled: true
    greeting: Hello, how can I help you today?
    input_encoding: ulaw
    input_gain_max_db: 0
    input_gain_target_rms: 0
    input_sample_rate_hz: 8000
    instructions: You are a voice assistant. Always speak your responses out loud.
    max_response_output_tokens: 4096
    model: gpt-4o-realtime-preview-2024-12-17
    organization: ''
    output_encoding: linear16
    output_sample_rate_hz: 24000
    provider_input_encoding: linear16
    provider_input_sample_rate_hz: 24000
    response_modalities:
      - audio
      - text
    target_encoding: mulaw
    target_sample_rate_hz: 8000
    temperature: 0.6
    turn_detection:
      create_response: true
      prefix_padding_ms: 300
      silence_duration_ms: 1000
      threshold: 0.5
      type: server_vad
    type: openai_realtime
    voice: alloy
  elevenlabs_agent:
    type: full
    enabled: true
    capabilities:
      - stt
      - llm
      - tts
    input_encoding: ulaw
    input_sample_rate_hz: 8000
    provider_input_encoding: pcm16
    provider_input_sample_rate_hz: 16000
    output_encoding: pcm16
    output_sample_rate_hz: 16000
    target_encoding: ulaw
    target_sample_rate_hz: 8000
    voice_id: uDsPstFWFBUXjIBimV7s
    model_id: eleven_flash_v2_5
    voice_settings:
      stability: 0.5
      similarity_boost: 0.75
      style: 0
      use_speaker_boost: true
    greeting: Hello! I'm your ElevenLabs voice assistant. How can I help you today?
    instructions: You are a helpful voice assistant. Be concise and friendly.
    continuous_input: true
    input_gain_target_rms: 0
    input_gain_max_db: 0
  openai_stt:
    chunk_size_ms: 20
    enabled: true
    input_encoding: linear16
    input_sample_rate_hz: 16000
    stt_model: whisper-1
    type: openai
  openai_tts:
    enabled: true
    target_encoding: mulaw
    target_sample_rate_hz: 8000
    tts_base_url: https://api.openai.com/v1/audio/speech
    tts_model: gpt-4o-mini-tts
    type: openai
    voice: alloy
streaming:
  chunk_size_ms: 20
  connection_timeout_ms: 120000
  continuous_stream: true
  diag_enable_taps: true
  diag_out_dir: /tmp/ai-engine-taps
  diag_post_secs: 1
  diag_pre_secs: 1
  empty_backoff_ticks_max: 5
  fallback_timeout_ms: 8000
  greeting_min_start_ms: 40
  jitter_buffer_ms: 950
  keepalive_interval_ms: 5000
  low_watermark_ms: 80
  min_start_ms: 120
  normalizer:
    enabled: true
    max_gain_db: 18
    target_rms: 1400
  provider_grace_ms: 200
  sample_rate: 8000
tools:
  ai_identity:
    name: AI Agent
    number: '6789'
  cancel_transfer:
    allow_after_answer: false
    allow_during_ring: true
    enabled: true
  default_action_timeout: 30
  enabled: true
  extensions:
    internal:
      '6000':
        action_type: transfer
        aliases:
          - agent
          - representative
          - human
          - real person
          - live person
          - someone
          - support
          - sales
          - operator
          - help desk
        description: Live customer service representative
        dial_string: SIP/6000
        mode: warm
        name: Live Agent
        pass_caller_info: true
        timeout: 30
        transfer: true
  hangup_call:
    enabled: true
    farewell_message: Thank you for calling. Goodbye!
    require_confirmation: false
  leave_voicemail:
    enabled: true
    extension: '2765'
  request_transcript:
    admin_email: haider@jugaar.llc
    api_key: ${RESEND_API_KEY}
    common_domains:
      - gmail.com
      - yahoo.com
      - outlook.com
      - hotmail.com
      - icloud.com
    confirm_email: true
    enabled: true
    from_email: ava@jugaar.llc
    from_name: AI Voice Agent
    max_attempts: 2
    provider: resend
    validate_domain: true
  send_email_summary:
    admin_email: haider@jugaar.llc
    api_key: ${RESEND_API_KEY}
    enabled: true
    from_email: ava@jugaar.llc
    from_name: AI Voice Agent
    include_metadata: true
    include_transcript: true
    provider: resend
  transfer:
    technology: SIP
    destinations:
      sales_agent:
        description: Sales agent
        target: '2765'
        type: extension
      sales_queue:
        description: Sales team queue
        target: '300'
        type: queue
      sales_team:
        description: Sales team ring group
        target: '600'
        type: ringgroup
      support_agent:
        description: Support agent
        target: '6000'
        type: extension
      support_queue:
        description: Technical support queue
        target: '301'
        type: queue
      support_team:
        description: Support team ring group
        target: '601'
        type: ringgroup
    enabled: true
mcp:
  enabled: true
  servers:
    weather:
      transport: stdio
      command:
        - python3
        - '-m'
        - src.mcp_servers.weather_mcp_server
      defaults:
        timeout_ms: 15000
        slow_response_threshold_ms: 3000
        slow_response_message: Let me check the weather for you, one moment...
      tools:
        - name: get_weather_by_city
          expose_as: mcp_weather_get_city
          speech_field: spoken
    aviation_atis:
      transport: stdio
      command:
        - python3
        - '-m'
        - src.mcp_servers.aviation_atis_server
        - '--config'
        - /app/config/aviation_atis.yaml
      env:
        METNO_USER_AGENT: >-
          Asterisk-AI-Voice-Agent
          (+https://github.com/hkjarral/Asterisk-AI-Voice-Agent)
      defaults:
        timeout_ms: 15000
        slow_response_threshold_ms: 3000
        slow_response_message: Let me get the current ATIS for you, one moment...
      tools:
        - name: get_atis
          expose_as: mcp_aviation_atis
          description: Get current ATIS for an ICAO airport code (e.g., LSMP, KJFK)
          speech_field: atis_text
vad:
  enhanced_enabled: true
  fallback_buffer_size: 128000
  fallback_enabled: true
  fallback_interval_ms: 4000
  max_utterance_duration_ms: 10000
  min_utterance_duration_ms: 600
  use_provider_vad: false
  utterance_padding_ms: 200
  webrtc_aggressiveness: 1
  webrtc_end_silence_frames: 50
  webrtc_start_frames: 3
